# LLM Integration Module

Architecture pour intÃ©gration LLM dans datannur via API Infomaniak (Qwen3 + Whisper)

## ğŸš€ Quick Start

### 1. Setup (browser console)

```javascript
// Configurer avec les credentials du POC
setupLLM()
```

Ou manuellement :

```javascript
localStorage.setItem('llm-api-key', 'your-api-key-here')
localStorage.setItem('llm-product-id', 'your-product-id')
```

### 2. Utiliser le composant

```svelte
<script>
  import LLMChat from '@llm/LLMChat.svelte'
</script>

<LLMChat />
```

## ğŸ“ Structure

```
src/llm/
â”œâ”€â”€ index.ts           # Exports publics
â”œâ”€â”€ llm-schema.ts      # Types TypeScript enrichis pour contexte LLM
â”œâ”€â”€ llm-context.ts     # Builder de contexte (DB stats + user + UI)
â”œâ”€â”€ llm-config.ts      # Configuration API Infomaniak
â”œâ”€â”€ llm-client.ts      # Client API avec streaming SSE
â”œâ”€â”€ llm-tools.ts       # Tools appelables par le LLM
â”œâ”€â”€ llm-setup.ts       # Helper pour setup rapide
â”œâ”€â”€ LLMChat.svelte     # Composant UI chat
â””â”€â”€ README.md          # Ce fichier
```

## ğŸ”‘ Configuration API Infomaniak

### Endpoints utilisÃ©s

**Chat Completions (Qwen3/Mixtral) :**

```
POST https://api.infomaniak.com/2/ai/{PRODUCT_ID}/openai/v1/chat/completions
```

**Speech-to-Text (Whisper) :**

```
POST https://api.infomaniak.com/1/ai/{PRODUCT_ID}/openai/audio/transcriptions
```

### Headers requis

```typescript
{
  'Content-Type': 'application/json',
  'Authorization': `Bearer ${API_KEY}`,
  'Accept': 'text/event-stream' // Pour streaming
}
```

### Credentials du POC

- **API Key:** ClÃ© fournie par Infomaniak
- **Product ID:** ID de votre produit Infomaniak

## ğŸ¯ Principe

### Single Source of Truth

Les **types TypeScript** servent Ã  la fois pour :

1. Type-checking du code de l'app
2. Documentation pour le LLM

**Avantages :**

- Pas de duplication avec JSON schemas
- Inclut les champs computed/processÃ©s
- Maintenance simplifiÃ©e
- Concis (Ã©conomie de tokens)

### Architecture Hybrid

```typescript
// 1. Context minimal (stats + user + UI)
const context = buildLLMContext()
// â†’ ~500-800 tokens

// 2. Tools pour queries dynamiques
const tools = getToolDefinitions()
// â†’ LLM appelle les fonctions selon besoin

// 3. SchÃ©mas en documentation
const systemPrompt = `
${getSchemaDocumentation()}
${getToolsDocumentation()}
`
```

## ğŸ“Š Types enrichis (llm-schema.ts)

Types avec documentation complÃ¨te :

```typescript
export type DatasetSchema = {
  // Raw database fields
  id: string
  name?: string
  type?: string
  nbRow?: number

  // Computed fields (added by app)
  typeClean?: string // Humanized
  folderName?: string // Denormalized
  nbVariable?: number // Calculated

  // Relations (loaded on demand)
  variables?: VariableSchema[]
  tags?: TagSchema[]

  // User context (runtime)
  isFavorite?: boolean
}
```

**Couverture complÃ¨te :**

- Dataset, Variable, Folder, Institution, Tag, Modality, Doc
- Champs bruts + computed + relations + user context
- Documentation inline (JSDoc)

## ğŸ”§ Context Builder (llm-context.ts)

```typescript
buildLLMContext({
  includeFullData: false, // true = ajoute samples
  entities: ['dataset'], // Limite les entitÃ©s
  maxItems: 20, // Limite par entitÃ©
})
```

**Retourne :**

- `stats` : Statistiques DB (counts, distributions)
- `user` : Favoris, historique recherche, prÃ©fÃ©rences
- `ui` : Page courante, filtres actifs
- `samples` : (optionnel) Exemples de donnÃ©es

**Optimisation tokens :**

- Minimal : ~500 tokens (stats only)
- Avec samples : ~2000 tokens (10 items/entitÃ©)

## ğŸ› ï¸ Tools (llm-tools.ts)

### Query Tools

- `findEntities` - Chercher entitÃ©s avec critÃ¨res
- `getEntity` - RÃ©cupÃ©rer entitÃ© par ID
- `countEntities` - Compter entitÃ©s
- `searchInCatalog` - Recherche full-text

### Analysis Tools

- `groupBy` - Grouper et compter par champ
- `getStatistics` - Stats numÃ©riques (mean, min, max, median)
- `getRelatedEntities` - RÃ©cupÃ©rer relations (variables d'un dataset, etc.)

### Navigation Tools

- `navigate` - Naviguer vers page

### User Data Tools

- `toggleFavorite` - GÃ©rer favoris

### Filter Tools

- `applyFilter` - Appliquer filtres
- `clearFilters` - Effacer filtres

**Format OpenAI Function Calling :**

```typescript
getToolDefinitions() // â†’ Array de tool definitions
executeTool(name, params) // â†’ Execute tool
```

## âš™ï¸ Configuration (llm-config.ts)

```typescript
// API Key stockÃ©e dans localStorage
setLLMAPIKey('sk-...')
isLLMConfigured() // â†’ boolean

// Config par dÃ©faut
{
  baseURL: 'https://api.infomaniak.com/v1',
  models: {
    text: 'qwen3',
    speech: 'whisper3'
  },
  maxTokens: 4096,
  temperature: 0.7
}
```

## ğŸ’¬ Utilisation

## ğŸ’¬ Utilisation du Chat

### Via le composant Svelte

```svelte
<script>
  import LLMChat from '@llm/LLMChat.svelte'
</script>

<LLMChat />
```

### Programmatique

```typescript
import { chat, chatStream } from '@llm'

// Simple (non-streaming)
const response = await chat([
  { role: 'user', content: 'Combien de datasets ?' },
])

// Streaming
await chatStream(
  [{ role: 'user', content: 'Liste les datasets panel' }],
  chunk => console.log(chunk), // Afficher au fur et Ã  mesure
)
```

### Avec tools

```typescript
import { chatCompletion, getToolDefinitions, executeTool } from '@llm'

await chatCompletion({
  messages: [{ role: 'user', content: 'Combien de datasets panel ?' }],
  tools: getToolDefinitions(),
  stream: true,
  onChunk: chunk => console.log(chunk),
  onToolCall: async toolCall => {
    const params = JSON.parse(toolCall.function.arguments)
    const result = executeTool(toolCall.function.name, params)
    console.log('Tool result:', result)
    return result
  },
})
```

## ğŸ“ Exemples de requÃªtes utilisateur

### Question simple

**User:** "Combien de datasets panel ?"

**LLM action:**

```json
{
  "tool": "countEntities",
  "params": {
    "entity": "dataset",
    "criteria": { "type": "panel" }
  }
}
```

### Question complexe

**User:** "Quels sont les datasets mis Ã  jour rÃ©cemment avec plus de 100 variables ?"

**LLM actions:**

1. `findEntities` avec criteria
2. Filter par nbVariable > 100
3. Sort par lastUpdateDate

### Navigation contextuelle

**User:** "Montre-moi ce dataset"

**LLM action:**

1. Utilise `ui.currentPage` pour identifier le dataset courant
2. `navigate` vers page dÃ©tail si pas dÃ©jÃ  dessus

### Analyse

**User:** "Quelle est la taille moyenne des datasets ?"

**LLM action:**

```json
{
  "tool": "getStatistics",
  "params": {
    "entity": "dataset",
    "field": "nbRow"
  }
}
```

## ğŸš€ Avantages de cette approche

### vs Extension VS Code

- âœ… Pas d'installation requise
- âœ… Fonctionne dans tout browser
- âœ… UI native dans l'app
- âœ… AccÃ¨s complet au contexte utilisateur

### vs SQL.js

- âœ… Pas de dÃ©pendance lourde
- âœ… DonnÃ©es dÃ©jÃ  processÃ©es/enrichies
- âœ… Types TypeScript = documentation vivante
- âœ… Tools plus expressifs que SQL

### vs JSON Schemas

- âœ… Single source of truth
- âœ… Inclut champs computed
- âœ… Plus concis (moins de tokens)
- âœ… Maintenance simplifiÃ©e

## ğŸ“ˆ Optimisations futures

1. **Caching** : Cache responses courantes
2. **Streaming** : SSE pour affichage progressif
3. **Context window** : RÃ©sumÃ© intelligent si DB trÃ¨s large
4. **Embeddings** : Pour recherche sÃ©mantique avancÃ©e
5. **User feedback** : Apprendre des corrections utilisateur

## ğŸ” SÃ©curitÃ©

- API key en localStorage (idÃ©alement chiffrÃ©)
- Validation des tool parameters
- Rate limiting cÃ´tÃ© client
- Pas de donnÃ©es sensibles dans prompts
